{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLOW-DC: Flexible Large-scale Orchestrated Workflow for Data Collection\n",
    "\n",
    "This notebook demonstrates the complete FLOW-DC pipeline for distributed image downloading:\n",
    "\n",
    "1. **Partition** - Split dataset manifest into partitions grouped by URL host\n",
    "2. **Estimate** - Calculate expected download size\n",
    "3. **Download** - Run distributed downloads with TaskVine\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "conda activate FLOW-DC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Verify we're in the right directory\n",
    "if not os.path.exists('../bin/download_batch.py'):\n",
    "    os.chdir('..')\n",
    "    \n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Input Data\n",
    "\n",
    "Verify that your input parquet file exists and has the expected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your input file\n",
    "INPUT_PARQUET = \"files/input/dataset.parquet\"  # Change this to your input file\n",
    "URL_COLUMN = \"url\"  # Change this to your URL column name\n",
    "LABEL_COLUMN = \"species\"  # Change this to your label column (or None)\n",
    "\n",
    "# Load and inspect\n",
    "df = pd.read_parquet(INPUT_PARQUET)\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nSample URLs:\")\n",
    "df[URL_COLUMN].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Estimate Download Size\n",
    "\n",
    "Use the CalcDatasetSize script to estimate how much storage you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$INPUT_PARQUET\" \"$URL_COLUMN\"\n",
    "# Sample 1000 URLs for quick estimation\n",
    "python bin/CalcDatasetSize.py --input \"$1\" --url_column \"$2\" --sample 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Partition Dataset by Host\n",
    "\n",
    "Split the dataset into partitions grouped by URL host. This optimizes distributed downloading by:\n",
    "- Keeping URLs from the same host in the same partition\n",
    "- Balancing partition sizes using greedy bin-packing\n",
    "- Enabling per-host rate limiting in PolicyBBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure partitioning\n",
    "NUM_PARTITIONS = 10  # Number of worker partitions\n",
    "OUTPUT_FOLDER = \"files/input/partitions\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$INPUT_PARQUET\" \"$URL_COLUMN\" \"$NUM_PARTITIONS\" \"$OUTPUT_FOLDER\"\n",
    "python bin/SplitParquet.py \\\n",
    "    --parquet \"$1\" \\\n",
    "    --url_col \"$2\" \\\n",
    "    --groups $3 \\\n",
    "    --output_folder \"$4\" \\\n",
    "    --method host \\\n",
    "    --add_host_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify partitions were created\n",
    "partitions = sorted([f for f in os.listdir(OUTPUT_FOLDER) if f.endswith('.parquet')])\n",
    "print(f\"Created {len(partitions)} partitions:\")\n",
    "for p in partitions:\n",
    "    pf = pd.read_parquet(os.path.join(OUTPUT_FOLDER, p))\n",
    "    hosts = pf['host'].nunique() if 'host' in pf.columns else 'N/A'\n",
    "    print(f\"  {p}: {len(pf):,} rows, {hosts} unique hosts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure TaskVine\n",
    "\n",
    "Create or update the TaskVine configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaskVine configuration\n",
    "config = {\n",
    "    \"port_number\": 9123,\n",
    "    \"parquets_directory\": OUTPUT_FOLDER,\n",
    "    \"output_directory\": \"files/output/images\",\n",
    "    \n",
    "    \"url_col\": URL_COLUMN,\n",
    "    \"label_col\": LABEL_COLUMN,\n",
    "    \n",
    "    \"concurrent_downloads\": 1000,\n",
    "    \"timeout_sec\": 30,\n",
    "    \"enable_polite_controller\": True,\n",
    "    \n",
    "    # PolicyBBR settings\n",
    "    \"initial_rate\": 100.0,\n",
    "    \"min_rate\": 1.0,\n",
    "    \"max_rate\": 10000.0,\n",
    "    \"per_host_conc_init\": 16,\n",
    "    \"per_host_conc_cap\": 512,\n",
    "    \n",
    "    # Task settings\n",
    "    \"max_retries\": 3,\n",
    "    \"timeout_minutes\": 60,\n",
    "    \"task_cores\": 4,\n",
    "    \"task_memory_mb\": 8000,\n",
    "    \n",
    "    # Output settings\n",
    "    \"create_tar\": True,\n",
    "    \"output_format\": \"imagefolder\"\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = \"files/config/taskvine_flowdc.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "    \n",
    "print(f\"Saved configuration to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start TaskVine Manager\n",
    "\n",
    "Run the TaskVine manager to distribute download tasks to workers.\n",
    "\n",
    "**Note:** You need to have TaskVine workers running and connected to the manager port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run to preview tasks\n",
    "!python bin/TaskvineFLOWDC.py --config files/config/taskvine_flowdc.json --dry_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the actual TaskVine manager (uncomment to run)\n",
    "# !python bin/TaskvineFLOWDC.py --config files/config/taskvine_flowdc.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Cloud Upload Mode\n",
    "\n",
    "For large datasets, use the cloud version that uploads directly to cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud configuration\n",
    "cloud_config = {\n",
    "    \"port_number\": 9123,\n",
    "    \"parquets_directory\": OUTPUT_FOLDER,\n",
    "    \n",
    "    \"cloud_destination\": \"AIIRA_Dataset\",  # Change to your destination\n",
    "    \"cloud_tool\": \"gocmd\",  # Options: gocmd, aws, gsutil, rclone\n",
    "    \n",
    "    \"url_col\": URL_COLUMN,\n",
    "    \"label_col\": LABEL_COLUMN,\n",
    "    \n",
    "    \"concurrent_downloads\": 1000,\n",
    "    \"enable_polite_controller\": True,\n",
    "    \n",
    "    \"max_retries\": 3,\n",
    "    \"timeout_minutes\": 120,\n",
    "    \"task_cores\": 8,\n",
    "    \"task_memory_mb\": 16000,\n",
    "}\n",
    "\n",
    "# Save cloud configuration\n",
    "cloud_config_path = \"files/config/taskvine_flowdc_cloud.json\"\n",
    "with open(cloud_config_path, 'w') as f:\n",
    "    json.dump(cloud_config, f, indent=2)\n",
    "    \n",
    "print(f\"Saved cloud configuration to {cloud_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run cloud tasks\n",
    "!python bin/TaskvineFLOWDCCloud.py --config files/config/taskvine_flowdc_cloud.json --dry_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Results\n",
    "\n",
    "After downloads complete, verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output directory\n",
    "output_dir = config[\"output_directory\"]\n",
    "if os.path.exists(output_dir):\n",
    "    tar_files = [f for f in os.listdir(output_dir) if f.endswith('.tar.gz')]\n",
    "    print(f\"Output tar files in {output_dir}:\")\n",
    "    for f in sorted(tar_files):\n",
    "        size_mb = os.path.getsize(os.path.join(output_dir, f)) / (1024 * 1024)\n",
    "        print(f\"  {f}: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"Output directory {output_dir} does not exist yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Machine Download (Alternative)\n",
    "\n",
    "For smaller datasets or testing, you can run downloads directly without TaskVine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single machine download (for testing or small datasets)\n",
    "# !python bin/download_batch.py --config files/config/gbif.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
