{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLOW-DC: Flexible Large-scale Orchestrated Workflow for Data Collection\n",
    "\n",
    "This notebook demonstrates the complete FLOW-DC pipeline for distributed image downloading:\n",
    "\n",
    "1. **Partition** - Split dataset manifest into partitions grouped by URL host\n",
    "2. **Estimate** - Calculate expected download size\n",
    "3. **Download** - Run distributed downloads with TaskVine\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "conda activate FLOW-DC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport polars as pl\n\n# Verify we're in the right directory\nif not os.path.exists('../bin/download_batch.py'):\n    os.chdir('..')\n    \nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Input Data\n",
    "\n",
    "Verify that your input parquet file exists and has the expected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure your input file\nINPUT_PARQUET = \"files/input/dataset.parquet\"  # Change this to your input file\nURL_COLUMN = \"url\"  # Change this to your URL column name\nLABEL_COLUMN = \"species\"  # Change this to your label column (or None)\n\n# Load and inspect with Polars\ndf = pl.read_parquet(INPUT_PARQUET)\nprint(f\"Loaded {df.height:,} rows\")\nprint(f\"\\nColumns: {df.columns}\")\nprint(f\"\\nSample URLs:\")\ndf.select(URL_COLUMN).head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Estimate Download Size\n",
    "\n",
    "Use the CalcDatasetSize script to estimate how much storage you'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$INPUT_PARQUET\" \"$URL_COLUMN\"\n",
    "# Sample 1000 URLs for quick estimation\n",
    "python bin/CalcDatasetSize.py --input \"$1\" --url_column \"$2\" --sample 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Partition Dataset by Host\n\nSplit the dataset into partitions grouped by URL host. This optimizes distributed downloading by:\n- Keeping URLs from the same host in the same partition\n- Balancing partition sizes using greedy bin-packing\n- Enabling per-host rate limiting with PAARC (Policy-Aware Adaptive Request Controller)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure partitioning\n",
    "NUM_PARTITIONS = 10  # Number of worker partitions\n",
    "OUTPUT_FOLDER = \"files/input/partitions\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$INPUT_PARQUET\" \"$URL_COLUMN\" \"$NUM_PARTITIONS\" \"$OUTPUT_FOLDER\"\n",
    "python bin/SplitParquet.py \\\n",
    "    --parquet \"$1\" \\\n",
    "    --url_col \"$2\" \\\n",
    "    --groups $3 \\\n",
    "    --output_folder \"$4\" \\\n",
    "    --method host \\\n",
    "    --add_host_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify partitions were created\npartitions = sorted([f for f in os.listdir(OUTPUT_FOLDER) if f.endswith('.parquet')])\nprint(f\"Created {len(partitions)} partitions:\")\nfor p in partitions:\n    pf = pl.read_parquet(os.path.join(OUTPUT_FOLDER, p))\n    hosts = pf.select('host').n_unique() if 'host' in pf.columns else 'N/A'\n    print(f\"  {p}: {pf.height:,} rows, {hosts} unique hosts\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure TaskVine\n",
    "\n",
    "Create or update the TaskVine configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TaskVine configuration with PAARC rate control\nconfig = {\n    \"port_number\": 9123,\n    \"parquets_directory\": OUTPUT_FOLDER,\n    \"output_directory\": \"files/output/images\",\n    \n    \"url_col\": URL_COLUMN,\n    \"label_col\": LABEL_COLUMN,\n    \n    # Download settings\n    \"concurrent_downloads\": 1000,\n    \"timeout_sec\": 30,\n    \n    # PAARC toggle\n    \"enable_paarc\": True,\n    \n    # PAARC concurrency bounds\n    \"C_init\": 8,\n    \"C_min\": 2,\n    \"C_max\": 2000,\n    \n    # PAARC utilization and backoff\n    \"mu\": 1.0,\n    \"beta\": 0.7,\n    \n    # PAARC latency thresholds\n    \"theta_50\": 1.5,\n    \"theta_95\": 2.0,\n    \"startup_theta_50\": 3.0,\n    \"startup_theta_95\": 4.0,\n    \n    # PAARC timing\n    \"probe_rtt_period\": 30.0,\n    \"rtprop_window\": 35.0,\n    \"cooldown_floor\": 2.0,\n    \n    # PAARC smoothing\n    \"alpha_ema\": 0.3,\n    \n    # Retry settings\n    \"max_retry_attempts\": 3,\n    \"retry_backoff_sec\": 2.0,\n    \n    # Task settings\n    \"timeout_minutes\": 60,\n    \"task_cores\": 4,\n    \"task_memory_mb\": 8000,\n    \n    # Output settings\n    \"create_tar\": True,\n    \"output_format\": \"imagefolder\"\n}\n\n# Save configuration\nconfig_path = \"files/config/taskvine_flowdc.json\"\nwith open(config_path, 'w') as f:\n    json.dump(config, f, indent=2)\n    \nprint(f\"Saved configuration to {config_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start TaskVine Manager\n",
    "\n",
    "Run the TaskVine manager to distribute download tasks to workers.\n",
    "\n",
    "**Note:** You need to have TaskVine workers running and connected to the manager port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run to preview tasks\n",
    "!python bin/TaskvineFLOWDC.py --config files/config/taskvine_flowdc.json --dry_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the actual TaskVine manager (uncomment to run)\n",
    "# !python bin/TaskvineFLOWDC.py --config files/config/taskvine_flowdc.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Cloud Upload Mode\n",
    "\n",
    "For large datasets, use the cloud version that uploads directly to cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cloud configuration with PAARC rate control\ncloud_config = {\n    \"port_number\": 9123,\n    \"parquets_directory\": OUTPUT_FOLDER,\n    \n    \"cloud_destination\": \"AIIRA_Dataset\",  # Change to your destination\n    \"cloud_tool\": \"gocmd\",  # Options: gocmd, aws, gsutil, rclone\n    \n    \"url_col\": URL_COLUMN,\n    \"label_col\": LABEL_COLUMN,\n    \n    # Download settings\n    \"concurrent_downloads\": 1000,\n    \"timeout_sec\": 30,\n    \n    # PAARC toggle\n    \"enable_paarc\": True,\n    \n    # PAARC concurrency bounds\n    \"C_init\": 8,\n    \"C_min\": 2,\n    \"C_max\": 2000,\n    \n    # PAARC utilization and backoff\n    \"mu\": 1.0,\n    \"beta\": 0.7,\n    \n    # Retry settings\n    \"max_retry_attempts\": 3,\n    \"retry_backoff_sec\": 2.0,\n    \n    # Task settings\n    \"timeout_minutes\": 120,\n    \"task_cores\": 8,\n    \"task_memory_mb\": 16000,\n}\n\n# Save cloud configuration\ncloud_config_path = \"files/config/taskvine_flowdc_cloud.json\"\nwith open(cloud_config_path, 'w') as f:\n    json.dump(cloud_config, f, indent=2)\n    \nprint(f\"Saved cloud configuration to {cloud_config_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry run cloud tasks\n",
    "!python bin/TaskvineFLOWDCCloud.py --config files/config/taskvine_flowdc_cloud.json --dry_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Results\n",
    "\n",
    "After downloads complete, verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output directory\n",
    "output_dir = config[\"output_directory\"]\n",
    "if os.path.exists(output_dir):\n",
    "    tar_files = [f for f in os.listdir(output_dir) if f.endswith('.tar.gz')]\n",
    "    print(f\"Output tar files in {output_dir}:\")\n",
    "    for f in sorted(tar_files):\n",
    "        size_mb = os.path.getsize(os.path.join(output_dir, f)) / (1024 * 1024)\n",
    "        print(f\"  {f}: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"Output directory {output_dir} does not exist yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Single-Machine Download (Alternative)\n\nFor smaller datasets or testing, you can run downloads directly without TaskVine.\nThis uses the PAARC (Policy-Aware Adaptive Request Controller) algorithm for per-host rate limiting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Single machine download (for testing or small datasets)\n# Uses PAARC rate control for adaptive per-host concurrency management\n# !python bin/download_batch.py --config files/config/gbif.json --enable_paarc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}